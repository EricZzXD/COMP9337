If the analyst is able somehow to get the source system to insert into the system
a message chosen by the analyst, then a chosen-plaintext attack is possible. An
example of this strategy is differential cryptanalysis, explored in Chapter 3. In general,
if the analyst is able to choose the messages to encrypt, the analyst may deliberately
pick patterns that can be expected to reveal the structure of the key.
Table 2.1 lists two other types of attack: chosen ciphertext and chosen text.
These are less commonly employed as cryptanalytic techniques but are nevertheless
possible avenues of attack.
Only relatively weak algorithms fail to withstand a ciphertext-only attack.
Generally, an encryption algorithm is designed to withstand a known-plaintext attack.
Two more definitions are worthy of note. An encryption scheme is unconditionally
secure if the ciphertext generated by the scheme does not contain enough
information to determine uniquely the corresponding plaintext, no matter how
much ciphertext is available. That is, no matter how much time an opponent has, it
is impossible for him or her to decrypt the ciphertext simply because the required
information is not there. With the exception of a scheme known as the one-time pad
(described later in this chapter), there is no encryption algorithm that is unconditionally
secure. Therefore, all that the users of an encryption algorithm can strive
for is an algorithm that meets one or both of the following criteria:
• The cost of breaking the cipher exceeds the value of the encrypted information.
• The time required to break the cipher exceeds the useful lifetime of the
information.
An encryption scheme is said to be computationally secure if either of the
foregoing two criteria are met. Unfortunately, it is very difficult to estimate the
amount of effort required to cryptanalyze ciphertext successfully.
All forms of cryptanalysis for symmetric encryption schemes are designed
to exploit the fact that traces of structure or pattern in the plaintext may survive
encryption and be discernible in the ciphertext. This will become clear as we examine
various symmetric encryption schemes in this chapter. We will see in Part Two
that cryptanalysis for public-key schemes proceeds from a fundamentally different
premise, namely, that the mathematical properties of the pair of keys may make it
possible for one of the two keys to be deduced from the other.
A brute-force attack involves trying every possible key until an intelligible
translation of the ciphertext into plaintext is obtained. On average, half of all possible
keys must be tried to achieve success. That is, if there are X different keys, on
average an attacker would discover the actual key after X>2 tries. It is important to
note that there is more to a brute-force attack than simply running through all possible
keys. Unless known plaintext is provided, the analyst must be able to recognize
plaintext as plaintext. If the message is just plain text in English, then the result pops
out easily, although the task of recognizing English would have to be automated. If
the text message has been compressed before encryption, then recognition is more
difficult. And if the message is some more general type of data, such as a numerical
file, and this has been compressed, the problem becomes even more difficult to
automate. Thus, to supplement the brute-force approach, some degree of knowledge
about the expected plaintext is needed, and some means of automatically
distinguishing plaintext from garble is also needed.Otherwise, each plaintext letter in a pair is replaced by the letter that lies in
its own row and the column occupied by the other plaintext letter. Thus, hs
becomes BP and ea becomes IM (or JM, as the encipherer wishes).
The Playfair cipher is a great advance over simple monoalphabetic ciphers.
For one thing, whereas there are only 26 letters, there are 26 * 26 = 676 digrams, so
that identification of individual digrams is more difficult. Furthermore, the relative
frequencies of individual letters exhibit a much greater range than that of digrams,
making frequency analysis much more difficult. For these reasons, the Playfair
cipher was for a long time considered unbreakable. It was used as the standard field
system by the British Army in World War I and still enjoyed considerable use by the
U.S. Army and other Allied forces during World War II.
Despite this level of confidence in its security, the Playfair cipher is relatively
easy to break, because it still leaves much of the structure of the plaintext language
intact. A few hundred letters of ciphertext are generally sufficient.
One way of revealing the effectiveness of the Playfair and other ciphers
is shown in Figure 2.6. The line labeled plaintext plots a typical frequency
distribution of the 26 alphabetic characters (no distinction between upper
and lower case) in ordinary text. This is also the frequency distribution of any
monoalphabetic substitution cipher, because the frequency values for individual
letters are the same, just with different letters substituted for the original letters.
The plot is developed in the following way: The number of occurrences of each
letter in the text is counted and divided by the number of occurrences of
the most frequently used letter. Using the results of Figure 2.5, we see that
e is the most frequently used letter. As a result, e has a relative frequency of 1, t of
If, on the other hand, a Vigenère cipher is suspected, then progress depends
on determining the length of the keyword, as will be seen in a moment. For now, let
us concentrate on how the keyword length can be determined. The important insight
that leads to a solution is the following: If two identical sequences of plaintext
letters occur at a distance that is an integer multiple of the keyword length, they will
generate identical ciphertext sequences. In the foregoing example, two instances
of the sequence “red” are separated by nine character positions. Consequently, in
both cases, r is encrypted using key letter e, e is encrypted using key letter p, and d
is encrypted using key letter t. Thus, in both cases, the ciphertext sequence is VTW.
We indicate this above by underlining the relevant ciphertext letters and shading
the relevant ciphertext numbers.
An analyst looking at only the ciphertext would detect the repeated sequences
VTW at a displacement of 9 and make the assumption that the keyword is either three
or nine letters in length. The appearance of VTW twice could be by chance and may
not reflect identical plaintext letters encrypted with identical key letters. However,
if the message is long enough, there will be a number of such repeated ciphertext
sequences. By looking for common factors in the displacements of the various
sequences, the analyst should be able to make a good guess of the keyword length.
Solution of the cipher now depends on an important insight. If the keyword
length is m, then the cipher, in effect, consists of m monoalphabetic substitution
ciphers. For example, with the keyword DECEPTIVE, the letters in positions 1, 10,
19, and so on are all encrypted with the same monoalphabetic cipher. Thus, we can
use the known frequency characteristics of the plaintext language to attack each of
the monoalphabetic ciphers separately.
The periodic nature of the keyword can be eliminated by using a nonrepeating
keyword that is as long as the message itself. Vigenère proposed what is referred to
as an autokey system, in which a keyword is concatenated with the plaintext itself to
provide a running key. For our example,
Although these techniques may seem archaic, they have contemporary equivalents.
[WAYN09] proposes hiding a message by using the least significant bits of
frames on a CD. For example, the Kodak Photo CD format’s maximum resolution
is 3096 * 6144 pixels, with each pixel containing 24 bits of RGB color information.
The least significant bit of each 24-bit pixel can be changed without greatly affecting
the quality of the image. The result is that you can hide a 130-kB message in a single
digital snapshot. There are now a number of software packages available that take
this type of approach to steganography.
Steganography has a number of drawbacks when compared to encryption.
It requires a lot of overhead to hide a relatively few bits of information, although
using a scheme like that proposed in the preceding paragraph may make it more
effective. Also, once the system is discovered, it becomes virtually worthless. This
problem, too, can be overcome if the insertion method depends on some sort of key
(e.g., see Problem 2.20). Alternatively, a message can be first encrypted and then
hidden using steganography.
The advantage of steganography is that it can be employed by parties who
have something to lose should the fact of their secret communication (not necessarily
the content) be discovered. Encryption flags traffic as important or secret or may
identify the sender or receiver as someone with something to hide.
For anyone interested in the history of code making and code breaking, the book to read is
[KAHN96]. Although it is concerned more with the impact of cryptology than its technical
development, it is an excellent introduction and makes for exciting reading. Another excellent
historical account is [SING99].
A short treatment covering the techniques of this chapter, and more, is [GARD72].
There are many books that cover classical cryptography in a more technical vein; one of the
best is [SINK09]. [KORN96] is a delightful book to read and contains a lengthy section on
classical techniques. Two cryptography books that contain a fair amount of technical material
on classical techniques are [GARR01] and [NICH99]. For the truly interested reader,
the two-volume [NICH96] covers numerous classical ciphers in detail and provides many
ciphertexts to be cryptanalyzed, together with the solutions.
An excellent treatment of rotor machines, including a discussion of their cryptanalysis
is found in [KUMA97].
[KATZ00] provides a thorough treatment of steganography. Another good source is
[WAYN09].
If the analyst is able somehow to get the source system to insert into the system
a message chosen by the analyst, then a chosen-plaintext attack is possible. An
example of this strategy is differential cryptanalysis, explored in Chapter 3. In general,
if the analyst is able to choose the messages to encrypt, the analyst may deliberately
pick patterns that can be expected to reveal the structure of the key.
Table 2.1 lists two other types of attack: chosen ciphertext and chosen text.
These are less commonly employed as cryptanalytic techniques but are nevertheless
possible avenues of attack.
Only relatively weak algorithms fail to withstand a ciphertext-only attack.
Generally, an encryption algorithm is designed to withstand a known-plaintext attack.
Two more definitions are worthy of note. An encryption scheme is unconditionally
secure if the ciphertext generated by the scheme does not contain enough
information to determine uniquely the corresponding plaintext, no matter how
much ciphertext is available. That is, no matter how much time an opponent has, it
is impossible for him or her to decrypt the ciphertext simply because the required
information is not there. With the exception of a scheme known as the one-time pad
(described later in this chapter), there is no encryption algorithm that is unconditionally
secure. Therefore, all that the users of an encryption algorithm can strive
for is an algorithm that meets one or both of the following criteria:
• The cost of breaking the cipher exceeds the value of the encrypted information.
• The time required to break the cipher exceeds the useful lifetime of the
information.
An encryption scheme is said to be computationally secure if either of the
foregoing two criteria are met. Unfortunately, it is very difficult to estimate the
amount of effort required to cryptanalyze ciphertext successfully.
All forms of cryptanalysis for symmetric encryption schemes are designed
to exploit the fact that traces of structure or pattern in the plaintext may survive
encryption and be discernible in the ciphertext. This will become clear as we examine
various symmetric encryption schemes in this chapter. We will see in Part Two
that cryptanalysis for public-key schemes proceeds from a fundamentally different
premise, namely, that the mathematical properties of the pair of keys may make it
possible for one of the two keys to be deduced from the other.
A brute-force attack involves trying every possible key until an intelligible
translation of the ciphertext into plaintext is obtained. On average, half of all possible
keys must be tried to achieve success. That is, if there are X different keys, on
average an attacker would discover the actual key after X>2 tries. It is important to
note that there is more to a brute-force attack than simply running through all possible
keys. Unless known plaintext is provided, the analyst must be able to recognize
plaintext as plaintext. If the message is just plain text in English, then the result pops
out easily, although the task of recognizing English would have to be automated. If
the text message has been compressed before encryption, then recognition is more
difficult. And if the message is some more general type of data, such as a numerical
file, and this has been compressed, the problem becomes even more difficult to
automate. Thus, to supplement the brute-force approach, some degree of knowledge
about the expected plaintext is needed, and some means of automatically
distinguishing plaintext from garble is also needed.
If, on the other hand, a Vigenère cipher is suspected, then progress depends
on determining the length of the keyword, as will be seen in a moment. For now, let
us concentrate on how the keyword length can be determined. The important insight
that leads to a solution is the following: If two identical sequences of plaintext
letters occur at a distance that is an integer multiple of the keyword length, they will
generate identical ciphertext sequences. In the foregoing example, two instances
of the sequence “red” are separated by nine character positions. Consequently, in
both cases, r is encrypted using key letter e, e is encrypted using key letter p, and d
is encrypted using key letter t. Thus, in both cases, the ciphertext sequence is VTW.
We indicate this above by underlining the relevant ciphertext letters and shading
the relevant ciphertext numbers.
An analyst looking at only the ciphertext would detect the repeated sequences
VTW at a displacement of 9 and make the assumption that the keyword is either three
or nine letters in length. The appearance of VTW twice could be by chance and may
not reflect identical plaintext letters encrypted with identical key letters. However,
if the message is long enough, there will be a number of such repeated ciphertext
sequences. By looking for common factors in the displacements of the various
sequences, the analyst should be able to make a good guess of the keyword length.
Solution of the cipher now depends on an important insight. If the keyword
length is m, then the cipher, in effect, consists of m monoalphabetic substitution
ciphers. For example, with the keyword DECEPTIVE, the letters in positions 1, 10,
19, and so on are all encrypted with the same monoalphabetic cipher. Thus, we can
use the known frequency characteristics of the plaintext language to attack each of
the monoalphabetic ciphers separately.
The periodic nature of the keyword can be eliminated by using a nonrepeating
keyword that is as long as the message itself. Vigenère proposed what is referred to
as an autokey system, in which a keyword is concatenated with the plaintext itself to
provide a running key. For our example,
Suppose that a cryptanalyst had managed to find these two keys. Two plausible
plaintexts are produced. How is the cryptanalyst to decide which is the correct
decryption (i.e., which is the correct key)? If the actual key were produced in a truly
random fashion, then the cryptanalyst cannot say that one of these two keys is more
likely than the other. Thus, there is no way to decide which key is correct and therefore
which plaintext is correct.
In fact, given any plaintext of equal length to the ciphertext, there is a key that
produces that plaintext. Therefore, if you did an exhaustive search of all possible
keys, you would end up with many legible plaintexts, with no way of knowing which
was the intended plaintext. Therefore, the code is unbreakable.
The security of the one-time pad is entirely due to the randomness of
the key. If the stream of characters that constitute the key is truly random, then the
stream of characters that constitute the ciphertext will be truly random. Thus, there
are no patterns or regularities that a cryptanalyst can use to attack the ciphertext.
In theory, we need look no further for a cipher. The one-time pad offers complete
security but, in practice, has two fundamental difficulties:
1. There is the practical problem of making large quantities of random keys.
Any heavily used system might require millions of random characters
on a regular basis. Supplying truly random characters in this volume is a
significant task.
2. Even more daunting is the problem of key distribution and protection. For
every message to be sent, a key of equal length is needed by both sender and
receiver. Thus, a mammoth key distribution problem exists.
Because of these difficulties, the one-time pad is of limited utility and is useful
primarily for low-bandwidth channels requiring very high security.
The one-time pad is the only cryptosystem that exhibits what is referred to as
perfect secrecy. This concept is explored in Appendix F.
For AES, DES, or any block cipher, encryption is performed on a block of b bits. In
the case of DES, b = 64 and in the case of AES, b = 128. However, it is possible
to convert a block cipher into a stream cipher, using one of the three modes to be
discussed in this and the next two sections: cipher feedback (CFB) mode, output
feedback (OFB) mode, and counter (CTR) mode. A stream cipher eliminates the
need to pad a message to be an integral number of blocks. It also can operate in
real time. Thus, if a character stream is being transmitted, each character can be
encrypted and transmitted immediately using a character-oriented stream cipher.
One desirable property of a stream cipher is that the ciphertext be of the same
length as the plaintext. Thus, if 8-bit characters are being transmitted, each character
should be encrypted to produce a ciphertext output of 8 bits. If more than 8 bits
are produced, transmission capacity is wasted.
Figure 6.5 depicts the CFB scheme. In the figure, it is assumed that the unit of
transmission is s bits; a common value is s = 8. As with CBC, the units of plaintext
are chained together, so that the ciphertext of any plaintext unit is a function of all
the preceding plaintext. In this case, rather than blocks of b bits, the plaintext is
divided into segments of s bits.
First, consider encryption. The input to the encryption function is a b-bit shift
register that is initially set to some initialization vector (IV). The leftmost (most
significant) s bits of the output of the encryption function are XORed with the
first segment of plaintext P1 to produce the first unit of ciphertext C1, which is then
transmitted. In addition, the contents of the shift register are shifted left by s bits,
and C1 is placed in the rightmost (least significant) s bits of the shift register. This
process continues until all plaintext units have been encrypted.
For decryption, the same scheme is used, except that the received ciphertext
unit is XORed with the output of the encryption function to produce the plaintext
unit. Note that it is the encryption function that is used, not the decryption function.
This is easily explained. Let MSBs(X) be defined as the most significant s bits
of X. Then
A popular approach to PRNG construction is to use a symmetric block cipher as
the heart of the PRNG mechanism. For any block of plaintext, a symmetric block
cipher produces an output block that is apparently random. That is, there are no
patterns or regularities in the ciphertext that provide information that can be used
to deduce the plaintext. Thus, a symmetric block cipher is a good candidate for
building a pseudorandom number generator.
If an established, standardized block cipher is used, such as DES or AES, then
the security characteristics of the PRNG can be established. Further, many applications
already make use of DES or AES, so the inclusion of the block cipher as part
of the PRNG algorithm is straightforward.
PRNG Using Block Cipher Modes of Operation
Two approaches that use a block cipher to build a PNRG have gained widespread
acceptance: the CTR mode and the OFB mode. The CTR mode is recommended in
NIST SP 800-90, in the ANSI standard X9.82 (Random Number Generation), and in
RFC 4086. The OFB mode is recommended in X9.82 and RFC 4086.
Figure 7.4 illustrates the two methods. In each case, the seed consists of two
parts: the encryption key value and a value V that will be updated after each block
of pseudorandom numbers is generated. Thus, for AES-128, the seed consists of a
128-bit key and a 128-bit V value. In the CTR case, the value of V is incremented by 1
after each encryption. In the case of OFB, the value of V is updated to equal the
either symmetric or public-key encryption that makes one superior to another from
the point of view of resisting cryptanalysis.
A second misconception is that public-key encryption is a general-purpose
technique that has made symmetric encryption obsolete. On the contrary, because
of the computational overhead of current public-key encryption schemes, there
seems no foreseeable likelihood that symmetric encryption will be abandoned. As
one of the inventors of public-key encryption has put it [DIFF88], “the restriction
of public-key cryptography to key management and signature applications is almost
universally accepted.”
Finally, there is a feeling that key distribution is trivial when using publickey
encryption, compared to the rather cumbersome handshaking involved with
key distribution centers for symmetric encryption. In fact, some form of protocol
is needed, generally involving a central agent, and the procedures involved are not
simpler nor any more efficient than those required for symmetric encryption (e.g.,
see analysis in [NEED78]).
This chapter and the next provide an overview of public-key cryptography.
First, we look at its conceptual framework. Interestingly, the concept for this
technique was developed and published before it was shown to be practical to
adopt it. Next, we examine the RSA algorithm, which is the most important encryption/
decryption algorithm that has been shown to be feasible for public-key
encryption. Other important public-key cryptographic algorithms are covered in
Chapter 10.
Much of the theory of public-key cryptosystems is based on number theory.
If one is prepared to accept the results given in this chapter, an understanding of
number theory is not strictly necessary. However, to gain a full appreciation of
public-key algorithms, some understanding of number theory is required. Chapter 8
provides the necessary background in number theory.
Table 9.1 defines some key terms.
The concept of public-key cryptography evolved from an attempt to attack two of
the most difficult problems associated with symmetric encryption. The first problem
is that of key distribution, which is examined in some detail in Chapter 14.
As Chapter 14 discusses, key distribution under symmetric encryption requires
either (1) that two communicants already share a key, which somehow has been distributed
to them; or (2) the use of a key distribution center. Whitfield Diffie, one
of the discoverers of public-key encryption (along with Martin Hellman, both at
Stanford University at the time), reasoned that this second requirement negated
the very essence of cryptography: the ability to maintain total secrecy over your
own communication. As Diffie put it [DIFF88], “what good would it do after all to
develop impenetrable cryptosystems, if their users were forced to share their keys
with a KDC that could be compromised by either burglary or subpoena?”
The second problem that Diffie pondered, and one that was apparently
unrelated to the first, was that of digital signatures. If the use of cryptography
was to become widespread, not just in military situations but for commercial and
private purposes, then electronic messages and documents would need the equivalent
of signatures used in paper documents. That is, could a method be devised
that would stipulate, to the satisfaction of all parties, that a digital message had
been sent by a particular person? This is a somewhat broader requirement than
that of authentication, and its characteristics and ramifications are explored in
Chapter 13.
Diffie and Hellman achieved an astounding breakthrough in 1976
[DIFF76 a, b] by coming up with a method that addressed both problems and was
radically different from all previous approaches to cryptography, going back over
four millennia.1
In the next subsection, we look at the overall framework for public-key
cryptography. Then we examine the requirements for the encryption/decryption
algorithm that is at the heart of the scheme.
As with symmetric encryption, a public-key encryption scheme is vulnerable to a
brute-force attack. The countermeasure is the same: Use large keys. However, there
is a tradeoff to be considered. Public-key systems depend on the use of some sort of
invertible mathematical function. The complexity of calculating these functions may
not scale linearly with the number of bits in the key but grow more rapidly than that.
Thus, the key size must be large enough to make brute-force attack impractical but
small enough for practical encryption and decryption. In practice, the key sizes that
have been proposed do make brute-force attack impractical but result in encryption/
decryption speeds that are too slow for general-purpose use. Instead, as was mentioned
earlier, public-key encryption is currently confined to key management and
signature applications.
Another form of attack is to find some way to compute the private key given
the public key. To date, it has not been mathematically proven that this form of attack
is infeasible for a particular public-key algorithm. Thus, any given algorithm,
including the widely used RSA algorithm, is suspect. The history of cryptanalysis
shows that a problem that seems insoluble from one perspective can be found to
have a solution if looked at in an entirely different way.
Finally, there is a form of attack that is peculiar to public-key systems. This is,
in essence, a probable-message attack. Suppose, for example, that a message were to
be sent that consisted solely of a 56-bit DES key. An adversary could encrypt all possible
56-bit DES keys using the public key and could discover the encrypted key by
matching the transmitted ciphertext. Thus, no matter how large the key size of the
public-key scheme, the attack is reduced to a brute-force attack on a 56-bit key. This
attack can be thwarted by appending some random bits to such simple messages.Most discussions of the cryptanalysis of RSA have focused on the task of factoring
n into its two prime factors. Determining f(n) given n is equivalent to factoring n
[RIBE96]. With presently known algorithms, determining d given e and n appears to
be at least as time-consuming as the factoring problem [KALI95]. Hence, we can use
factoring performance as a benchmark against which to evaluate the security of RSA.
For a large n with large prime factors, factoring is a hard problem, but it is
not as hard as it used to be. A striking illustration of this is the following. In 1977,
the three inventors of RSA dared Scientific American readers to decode a cipher
they printed in Martin Gardner’s “Mathematical Games” column [GARD77]. They
offered a $100 reward for the return of a plaintext sentence, an event they predicted
might not occur for some 40 quadrillion years. In April of 1994, a group working
over the Internet claimed the prize after only eight months of work [LEUT94]. This
challenge used a public key size (length of n) of 129 decimal digits, or around 428
bits. In the meantime, just as they had done for DES, RSA Laboratories had issued
challenges for the RSA cipher with key sizes of 100, 110, 120, and so on, digits. The
latest challenge to be met is the RSA-768 challenge with a key length of 232 decimal
digits, or 768 bits. Table 9.5 shows the results to date. Million-instructions-per-second
processor running for one year, which is about 3 * 1013 instructions executed.
A 1 GHz Pentium is about a 250-MIPS machine.
A striking fact about the progress reflected in Table 9.5 concerns the method
used. Until the mid-1990s, factoring attacks were made using an approach known
as the quadratic sieve. The attack on RSA-130 used a newer algorithm, the generalized
number field sieve (GNFS), and was able to factor a larger number than RSA-
129 at only 20% of the computing effort.
The threat to larger key sizes is twofold: the continuing increase in computing
power and the continuing refinement of factoring algorithms. We have seen
that the move to a different algorithm resulted in a tremendous speedup. We
can expect further refinements in the GNFS, and the use of an even better algorithm
is also a possibility. In fact, a related algorithm, the special number field
In addition, it has been demonstrated that if e 6 n and d 6 n1/4, then d can be
easily determined [WIEN90].
Timing Attacks If one needed yet another lesson about how difficult it is to
assess the security of a cryptographic algorithm, the appearance of timing attacks
provides a stunning one. Paul Kocher, a cryptographic consultant, demonstrated
that a snooper can determine a private key by keeping track of how long a computer
takes to decipher messages [KOCH96, KALI96b]. Timing attacks are applicable
not just to RSA, but to other public-key cryptography systems. This attack is alarming
for two reasons: It comes from a completely unexpected direction, and it is a
ciphertext-only attack.
A timing attack is somewhat analogous to a burglar guessing the combination
of a safe by observing how long it takes for someone to turn the dial from number
to number. We can explain the attack using the modular exponentiation algorithm
of Figure 9.8, but the attack can be adapted to work with any implementation that
does not run in fixed time. In this algorithm, modular exponentiation is accomplished
bit by bit, with one modular multiplication performed at each iteration and
an additional modular multiplication performed for each 1 bit.
As Kocher points out in his paper, the attack is simplest to understand in an
extreme case. Suppose the target system uses a modular multiplication function that
is very fast in almost all cases but in a few cases takes much more time than an entire
average modular exponentiation. The attack proceeds bit-by-bit starting with the
leftmost bit, bk. Suppose that the first j bits are known (to obtain the entire exponent,
start with j = 0 and repeat the attack until the entire exponent is known). For
a given ciphertext, the attacker can complete the first j iterations of the for loop.
The operation of the subsequent step depends on the unknown exponent bit. If the
bit is set, d d (d * a) mod n will be executed. For a few values of a and d, the modular
multiplication will be extremely slow, and the attacker knows which these are.
Therefore, if the observed time to execute the decryption algorithm is always slow
when this particular iteration is slow with a 1 bit, then this bit is assumed to be 1. If
a number of observed execution times for the entire algorithm are fast, then this bit
is assumed to be 0.Therefore, Y = (2M) mod n. From this, we can deduce M. To overcome this
simple attack, practical RSA-based cryptosystems randomly pad the plaintext prior
to encryption. This randomizes the ciphertext so that Equation (9.2) no longer
holds. However, more sophisticated CCAs are possible, and a simple padding with a
random value has been shown to be insufficient to provide the desired security. To
counter such attacks, RSA SecurityInc., a leading RSA vendor and former holder
of the RSA patent, recommends modifying the plaintext using a procedure known
as optimal asymmetric encryption padding (OAEP). A full discussion of the thre